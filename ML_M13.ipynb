{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccf678-7b0b-454e-9b8a-0c00aac8d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "1Ô∏è‚É£ What is unsupervised learning in the context of machine learning?\n",
    "Unsupervised learning is a type of machine learning where the model learns patterns and structures in unlabeled data. Unlike supervised learning (with input-output pairs), unsupervised learning finds hidden structures (like clusters, groups, or associations) within data without predefined labels. Examples include clustering and dimensionality reduction.\n",
    "\n",
    "2Ô∏è‚É£ How does K-Means clustering algorithm work?\n",
    "K-Means partitions data into K clusters by:\n",
    "\n",
    "Randomly initializing K cluster centroids.\n",
    "\n",
    "Assigning each point to the nearest centroid (based on distance).\n",
    "\n",
    "Updating centroids by calculating the mean of points in each cluster.\n",
    "\n",
    "Repeating steps 2-3 until centroids stabilize or max iterations are reached.\n",
    "\n",
    "3Ô∏è‚É£ Explain the concept of a dendrogram in hierarchical clustering.\n",
    "A dendrogram is a tree-like diagram that shows how data points are merged in hierarchical clustering. It starts with each point as its own cluster and merges them step by step, showing cluster hierarchy. Cutting the dendrogram at a chosen height gives the final clusters.\n",
    "\n",
    "4Ô∏è‚É£ What is the main difference between K-Means and Hierarchical Clustering?\n",
    "K-Means: Requires the number of clusters (K) in advance; forms non-overlapping clusters.\n",
    "\n",
    "Hierarchical: Builds a hierarchy (dendrogram); doesn't need K in advance; can give nested clusters at different levels.\n",
    "\n",
    "5Ô∏è‚É£ What are the advantages of DBSCAN over K-Means?\n",
    "Can find arbitrarily shaped clusters.\n",
    "\n",
    "Automatically detects outliers (noise).\n",
    "\n",
    "No need to specify the number of clusters in advance.\n",
    "\n",
    "Handles varying densities better than K-Means.\n",
    "\n",
    "6Ô∏è‚É£ When would you use Silhouette Score in clustering?\n",
    "Use Silhouette Score to evaluate how well points are clustered by measuring cohesion (within-cluster) and separation (between-cluster). It‚Äôs useful when comparing different clustering models or when the true labels are unknown.\n",
    "\n",
    "7Ô∏è‚É£ What are the limitations of Hierarchical Clustering?\n",
    "Computationally expensive for large datasets (O(n¬≤) complexity).\n",
    "\n",
    "No way to \"undo\" a merge step.\n",
    "\n",
    "Sensitive to noise and outliers.\n",
    "\n",
    "Less flexible for large or streaming data.\n",
    "\n",
    "8Ô∏è‚É£ Why is feature scaling important in clustering algorithms like K-Means?\n",
    "K-Means relies on distance metrics (e.g., Euclidean). Features with large magnitudes dominate the distance calculation, so scaling (like standardization) ensures all features contribute equally to the cluster formation.\n",
    "\n",
    "9Ô∏è‚É£ How does DBSCAN identify noise points?\n",
    "DBSCAN labels points as noise if they have fewer than min_samples points in their neighborhood (within distance Œµ). These points don‚Äôt belong to any cluster.\n",
    "\n",
    "üîü Define inertia in the context of K-Means.\n",
    "Inertia is the sum of squared distances between data points and their assigned cluster centroids. It measures how tightly points are clustered around centroids. Lower inertia suggests better clustering.\n",
    "\n",
    "1Ô∏è‚É£1Ô∏è‚É£ What is the elbow method in K-Means clustering?\n",
    "The elbow method involves plotting inertia vs. the number of clusters (K). The \"elbow point\" where inertia starts decreasing more slowly suggests the optimal K value.\n",
    "\n",
    "1Ô∏è‚É£2Ô∏è‚É£ Describe the concept of \"density\" in DBSCAN.\n",
    "Density refers to the number of points within a certain Œµ-neighborhood. DBSCAN groups points with enough neighbors (min_samples) into a cluster, forming clusters of high density, while sparse regions are considered noise.\n",
    "\n",
    "1Ô∏è‚É£3Ô∏è‚É£ Can hierarchical clustering be used on categorical data?\n",
    "Yes, but with modifications. You need to define a suitable distance metric for categorical data (e.g., Hamming distance or matching coefficient), as standard Euclidean distance isn‚Äôt meaningful for categories.\n",
    "\n",
    "1Ô∏è‚É£4Ô∏è‚É£ What does a negative Silhouette Score indicate?\n",
    "A negative score means a point is closer to points in a different cluster than to its own cluster‚Äîindicating poor clustering.\n",
    "\n",
    "1Ô∏è‚É£5Ô∏è‚É£ Explain the term \"linkage criteria\" in hierarchical clustering.\n",
    "Linkage criteria define how distances between clusters are measured when merging:\n",
    "\n",
    "Single linkage: Minimum distance.\n",
    "\n",
    "Complete linkage: Maximum distance.\n",
    "\n",
    "Average linkage: Average distance.\n",
    "\n",
    "1Ô∏è‚É£6Ô∏è‚É£ Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
    "K-Means assumes clusters are spherical and similar in size. If clusters vary in size/density, K-Means may assign points incorrectly, merge small clusters into large ones, or split dense clusters.\n",
    "\n",
    "1Ô∏è‚É£7Ô∏è‚É£ What are the core parameters in DBSCAN, and how do they influence clustering?\n",
    "Œµ (epsilon): Defines the radius for neighborhood searches.\n",
    "\n",
    "min_samples: Minimum points to form a dense region.\n",
    "A small Œµ forms many small clusters, large Œµ may merge clusters. The right choice of Œµ and min_samples is critical for effective clustering.\n",
    "\n",
    "1Ô∏è‚É£8Ô∏è‚É£ How does K-Means++ improve upon standard K-Means initialization?\n",
    "K-Means++ selects initial centroids by:\n",
    "\n",
    "Choosing the first centroid randomly.\n",
    "\n",
    "Selecting subsequent centroids based on the distance from already chosen centroids.\n",
    "This avoids poor initializations and speeds up convergence.\n",
    "\n",
    "1Ô∏è‚É£9Ô∏è‚É£ What is agglomerative clustering?\n",
    "Agglomerative clustering is a type of hierarchical clustering where:\n",
    "\n",
    "Each data point starts as its own cluster.\n",
    "\n",
    "Pairs of clusters are merged step by step based on a linkage criterion until all points form a single cluster.\n",
    "\n",
    "2Ô∏è‚É£0Ô∏è‚É£ What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
    "Inertia measures within-cluster compactness but not how clusters are separated.\n",
    "\n",
    "Silhouette Score combines cohesion (how similar points are to their own cluster) and separation (how different they are from other clusters), giving a more comprehensive evaluation, especially for comparing different cluster counts or models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f6168-fe41-48ca-86cf-c077a0dc4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "2Ô∏è‚É£1Ô∏è‚É£ Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
    "plt.title(\"K-Means Clustering with 4 Centers\")\n",
    "plt.show()\n",
    "2Ô∏è‚É£2Ô∏è‚É£ Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "labels = agg_clustering.fit_predict(X)\n",
    "\n",
    "# Display the first 10 predicted labels\n",
    "print(\"First 10 predicted labels:\", labels[:10])\n",
    "2Ô∏è‚É£3Ô∏è‚É£ Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=0)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Identify core samples and outliers\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "outliers = labels == -1\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X[~outliers, 0], X[~outliers, 1], c=labels[~outliers], cmap='viridis', s=50)\n",
    "plt.scatter(X[outliers, 0], X[outliers, 1], c='red', s=50, marker='x', label='Outliers')\n",
    "plt.title(\"DBSCAN Clustering with Outliers Highlighted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "2Ô∏è‚É£4Ô∏è‚É£ Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Print the size of each cluster\n",
    "cluster_sizes = Counter(labels)\n",
    "print(\"Cluster sizes:\", cluster_sizes)\n",
    "2Ô∏è‚É£5Ô∏è‚É£ Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=0)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title(\"DBSCAN Clustering on Concentric Circles\")\n",
    "plt.show()\n",
    "2Ô∏è‚É£6Ô∏è‚É£ Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Output the cluster centroids\n",
    "print(\"Cluster centroids:\\n\", kmeans.cluster_centers_)\n",
    "2Ô∏è‚É£7Ô∏è‚É£ Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN\n",
    "\n",
    "# Generate synthetic data with varying cluster standard deviations\n",
    "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[0.5, 1.0, 2.5], random_state=0)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.9, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title(\"DBSCAN Clustering with Varying Cluster Densities\")\n",
    "plt.show()\n",
    "2Ô∏è‚É£8Ô∏è‚É£ Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=50)\n",
    "plt.title(\"K-Means Clustering on Digits Dataset (PCA Reduced)\")\n",
    "plt.show()\n",
    "2Ô∏è‚É£9Ô∏è‚É£ Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Evaluate silhouette scores for k = 2 to 5\n",
    "scores = []\n",
    "k_values = range(2, 6)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    scores.append(score)\n",
    "\n",
    "# Display as a bar chart\n",
    "plt.bar(k_values, scores)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Scores for Different k Values\")\n",
    "plt.show()\n",
    "3Ô∏è‚É£0Ô∏è‚É£ Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Compute the linkage matrix\n",
    "linked = linkage(X, method='average')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, labels=iris.target, leaf_rotation=90)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram (Average Linkage)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "3Ô∏è‚É£6Ô∏è‚É£ Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Load and scale the Wine dataset\n",
    "data = load_wine()\n",
    "X = StandardScaler().fit_transform(data.data)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Count clusters (excluding noise)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(\"Number of clusters (excluding noise):\", n_clusters)\n",
    "3Ô∏è‚É£7Ô∏è‚É£ Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Plot data points and cluster centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
    "plt.title(\"KMeans Clustering with Cluster Centers\")\n",
    "plt.show()\n",
    "3Ô∏è‚É£8Ô∏è‚É£ Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load Iris dataset\n",
    "X = load_iris().data\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Count noise points\n",
    "noise_points = np.sum(labels == -1)\n",
    "print(\"Number of noise points:\", noise_points)\n",
    "3Ô∏è‚É£9Ô∏è‚É£ Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate non-linear data\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot clustering result\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title(\"KMeans Clustering on Non-Linear Data (make_moons)\")\n",
    "plt.show()\n",
    "4Ô∏è‚É£0Ô∏è‚É£ Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Load Digits dataset\n",
    "X = load_digits().data\n",
    "\n",
    "# Reduce to 3 components using PCA\n",
    "X_pca = PCA(n_components=3).fit_transform(X)\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# 3D Scatter plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='tab10', s=50)\n",
    "legend = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "ax.add_artist(legend)\n",
    "plt.title(\"3D PCA + KMeans on Digits Dataset\")\n",
    "plt.show()\n",
    "\n",
    "3Ô∏è‚É£1Ô∏è‚É£ Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generate blobs\n",
    "X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "score = silhouette_score(X, labels)\n",
    "print(\"Silhouette Score:\", score)\n",
    "3Ô∏è‚É£2Ô∏è‚É£ Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess\n",
    "X = load_breast_cancer().data\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# Agglomerative Clustering\n",
    "agg = AgglomerativeClustering(n_clusters=2)\n",
    "labels = agg.fit_predict(X)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Agglomerative Clustering on Breast Cancer (PCA Reduced)\")\n",
    "plt.show()\n",
    "3Ô∏è‚É£3Ô∏è‚É£ Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "# KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42).fit_predict(X)\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5).fit_predict(X)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=kmeans, cmap='viridis')\n",
    "ax[0].set_title(\"KMeans Clustering\")\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=dbscan, cmap='viridis')\n",
    "ax[1].set_title(\"DBSCAN Clustering\")\n",
    "plt.show()\n",
    "3Ô∏è‚É£4Ô∏è‚É£ Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "X = load_iris().data\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Silhouette Coefficients\n",
    "sil_scores = silhouette_samples(X, labels)\n",
    "\n",
    "# Plot\n",
    "plt.bar(range(len(sil_scores)), sil_scores)\n",
    "plt.title(\"Silhouette Coefficients per Sample (Iris Dataset)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()\n",
    "3Ô∏è‚É£5Ô∏è‚É£ Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
    "labels = agg.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Agglomerative Clustering ('average' linkage)\")\n",
    "plt.show()\n",
    "3Ô∏è‚É£6Ô∏è‚É£ Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data[:, :4], columns=wine.feature_names[:4])\n",
    "\n",
    "# KMeans clustering\n",
    "df['Cluster'] = KMeans(n_clusters=3, random_state=42).fit_predict(wine.data)\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(df, hue='Cluster', palette='viridis', diag_kind='kde')\n",
    "plt.show()\n",
    "3Ô∏è‚É£7Ô∏è‚É£ Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.5, random_state=42)\n",
    "\n",
    "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Count clusters and noise\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "print(\"Number of clusters:\", n_clusters)\n",
    "print(\"Number of noise points:\", n_noise)\n",
    "3Ô∏è‚É£8Ô∏è‚É£ Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = load_digits().data\n",
    "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=10)\n",
    "labels = agg.fit_predict(X)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=40)\n",
    "plt.title(\"Agglomerative Clustering on Digits (t-SNE Reduced)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
